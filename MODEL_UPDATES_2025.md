# üöÄ Model Updates 2025 - Multi-Provider CLI Chat Plugin

**Letzte Aktualisierung**: 21. November 2025

> ‚ö†Ô∏è **WICHTIG - Tats√§chlich verf√ºgbare Modelle im Codex CLI**:
>
> Die in diesem Dokument recherchierten Modelle (o3-pro, o4-mini, etc.) sind **NICHT** direkt im Codex CLI verf√ºgbar.
>
> **Tats√§chlich verf√ºgbare Modelle** (Stand November 2025):
> - `gpt-5.1-codex` - Optimized for codex (EMPFOHLEN f√ºr Coding)
> - `gpt-5.1-codex-mini` - Optimized for codex, cheaper & faster
> - `gpt-5.1` - (default) Broad world knowledge, general reasoning
>
> Die folgenden Abschnitte dienen als Hintergrundinformation zu OpenAI's Modell-Entwicklung.

## üìä OpenAI Modell-Entwicklung 2025 (Hintergrundinformation)

### üîµ OpenAI Models

#### o3-pro (Released: Juni 2025)
- **Modell-Name**: `o3-pro`
- **Typ**: Reasoning Model
- **Hauptmerkmale**:
  - Most capable reasoning model von OpenAI
  - Denkt l√§nger und liefert zuverl√§ssigste Antworten
  - 20% weniger schwere Fehler als o1
  - **Spezialisierung**: Programming, Business/Consulting, Creative Ideation
- **Benchmark-Highlights**:
  - SWE-bench Verified: 71.7% (vs. 48.9% bei o1)
  - Codeforces Elo: 2727 (vs. 1891 bei o1)
- **Empfohlen f√ºr**:
  - ‚úÖ Komplexe Coding-Aufgaben
  - ‚úÖ Software Engineering (h√∂chste SWE-bench-Score)
  - ‚úÖ Kritische Aufgaben, bei denen Zuverl√§ssigkeit wichtiger als Speed ist
- **Hinweis**: L√§ngere Response-Time (Background-Mode empfohlen)

#### o4-mini (Released: April 2025)
- **Modell-Name**: `o4-mini`
- **Typ**: Fast Reasoning Model
- **Hauptmerkmale**:
  - Erstes o4-Modell
  - Verbesserte Performance gegen√ºber o3-mini in allen Key-Benchmarks
  - Optimiert f√ºr Math, Coding, Visual Tasks
  - **Beste Kosten-Performance f√ºr technische Tasks**
- **Empfohlen f√ºr**:
  - ‚úÖ Schnelle Coding-Aufgaben
  - ‚úÖ Math/Science-Probleme
  - ‚úÖ Wenn Speed wichtig ist
  - ‚úÖ Budget-bewusste Projekte

#### gpt-5.1 (Released: August 2025)
- **Modell-Name**: `gpt-5.1`
- **Typ**: Flagship GPT Model
- **Hauptmerkmale**:
  - Neuestes GPT-Generation-Modell
  - Default f√ºr alle logged-in/out User
  - Verbesserte Coding-Personality & Code-Qualit√§t
  - Entwickelt in Zusammenarbeit mit: Cursor, Cognition, Augment Code, Factory, Warp
- **Empfohlen f√ºr**:
  - ‚úÖ Allgemeine Aufgaben
  - ‚úÖ Conversational AI
  - ‚úÖ Code-Generierung mit gutem Stil
  - ‚úÖ Balanced Performance

---

### üü¢ Google Gemini Models

#### gemini-3-pro-preview-11-2025 (Released: November 2025)
- **Modell-Name**: `gemini-3-pro-preview-11-2025`
- **Typ**: Most Intelligent Gemini Model
- **Hauptmerkmale**:
  - **Erste KI mit >1500 Elo auf LMArena** (1501 Elo)
  - 1 Million Token Input Context
  - 64k Token Output
  - Knowledge Cutoff: Januar 2025
  - State-of-the-art Reasoning + Multimodal Understanding + Agentic Capabilities
- **Benchmark-Highlights**:
  - Erster 1500+ Elo Score in der AI-Geschichte
  - Record-breaking Benchmark-Scores
- **Empfohlen f√ºr**:
  - ‚úÖ H√∂chste Intelligenz-Anforderungen
  - ‚úÖ Sehr gro√üe Contexts (bis 1M Token)
  - ‚úÖ Multimodale Aufgaben
  - ‚úÖ Agentic Workflows
  - ‚úÖ Complex Reasoning

#### gemini-3-pro-preview-11-2025-thinking (Released: November 2025)
- **Modell-Name**: `gemini-3-pro-preview-11-2025-thinking`
- **Typ**: Gemini 3 mit Reasoning-Visualisierung
- **Hauptmerkmale**:
  - Gleiche Capabilities wie Gemini 3 Pro
  - **Zeigt Denkprozess sichtbar an**
  - "Generative Interfaces" - Model w√§hlt beste Output-Form
- **Empfohlen f√ºr**:
  - ‚úÖ Debugging komplexer Logik
  - ‚úÖ Verst√§ndnis des AI-Reasoning
  - ‚úÖ Educational Use Cases
  - ‚úÖ Transparenz-Anforderungen

#### gemini-3.0-flash (Released: November 2025)
- **Modell-Name**: `gemini-3.0-flash`
- **Typ**: Fast Gemini 3 Variant
- **Hauptmerkmale**:
  - Distilled, latency-focused Version
  - **Sub-Sekunden Response-Times**
  - Hohe Capability bei extremer Geschwindigkeit
- **Empfohlen f√ºr**:
  - ‚úÖ Latenz-kritische Anwendungen
  - ‚úÖ Real-time Interactions
  - ‚úÖ High-throughput Scenarios
  - ‚úÖ Schnelle Prototyping-Iterationen

---

## üìà Modell-Vergleich & Use-Case-Matrix

### Coding-Aufgaben

| Use Case | Beste Wahl | Alternative | Begr√ºndung |
|----------|-----------|-------------|------------|
| **Komplexe Software Engineering** | `o3-pro` | `gemini-3-pro-preview` | H√∂chste SWE-bench-Score (71.7%) |
| **Schnelle Code-Generierung** | `o4-mini` | `gemini-3.0-flash` | Optimiert f√ºr Speed + Quality |
| **Code-Review & Refactoring** | `gemini-3-pro-preview` | `o3-pro` | 1M Token Context f√ºr gro√üe Codebases |
| **Bug-Fixing** | `o4-mini` | `gpt-5.1` | Schnelle Iteration wichtig |
| **Architecture Design** | `o3-pro` | `gemini-3-pro-preview-thinking` | Komplexes Reasoning erforderlich |

### Allgemeine Aufgaben

| Use Case | Beste Wahl | Alternative | Begr√ºndung |
|----------|-----------|-------------|------------|
| **Conversational AI** | `gpt-5.1` | `gemini-3-pro-preview` | Beste Personality |
| **Dokumentation** | `gemini-3-pro-preview` | `gpt-5.1` | 1M Context f√ºr gro√üe Docs |
| **Schnelle Q&A** | `gemini-3.0-flash` | `o4-mini` | Sub-Sekunden-Antworten |
| **Complex Reasoning** | `o3-pro` | `gemini-3-pro-preview` | H√∂chste Reasoning-Qualit√§t |
| **Multimodal Tasks** | `gemini-3-pro-preview` | - | Native Multimodal Support |

### Performance vs. Quality

```
H√∂chste Qualit√§t (langsamer):
  o3-pro > gemini-3-pro-preview > o3-mini > gpt-5.1

Beste Balance:
  o4-mini > gemini-3.0-flash > gpt-5.1 > gemini-2.5-pro

H√∂chste Geschwindigkeit:
  gemini-3.0-flash > o4-mini > gpt-5.1 > o3-mini
```

---

## üéØ Praktische Verwendungsbeispiele

### OpenAI/Codex Commands

#### Hochkomplexe Software-Entwicklung
```bash
/openai-cli {
  "prompt": "Entwirf eine skalierbare Microservices-Architektur f√ºr ein E-Commerce-System mit Event Sourcing",
  "model": "o3-pro",
  "sandbox": "danger-full-access",
  "approval_policy": "never"
}
```

#### Schnelle Code-Generierung
```bash
/openai-cli {
  "prompt": "Schreibe eine Python-Funktion f√ºr Binary Search mit Tests",
  "model": "o4-mini",
  "sandbox": "workspace-write",
  "approval_policy": "on-failure"
}
```

#### Allgemeine Aufgaben
```bash
/openai-cli {
  "prompt": "Erkl√§re mir REST vs GraphQL f√ºr eine API-Entscheidung",
  "model": "gpt-5.1"
}
```

---

### Gemini Commands

#### Gro√üe Codebase-Analyse
```bash
/gemini-cli {
  "prompt": "Analysiere diese gesamte Codebase und identifiziere Verbesserungspotential",
  "model": "gemini-3-pro-preview-11-2025",
  "approval_mode": "yolo"
}
```

#### Reasoning mit Transparenz
```bash
/gemini-cli {
  "prompt": "L√∂se dieses komplexe Algorithm-Problem und zeige deinen Denkprozess",
  "model": "gemini-3-pro-preview-11-2025-thinking",
  "approval_mode": "default"
}
```

#### Ultra-schnelle Responses
```bash
/gemini-cli {
  "prompt": "Quick: Was ist der Unterschied zwischen Array und Linked List?",
  "model": "gemini-3.0-flash",
  "yolo": true
}
```

---

## üîÑ Migration von √§lteren Modellen

### Von o3-mini zu o4-mini
**Warum wechseln?**
- o4-mini √ºbertrifft o3-mini in allen Key-Benchmarks
- Bessere Performance bei √§hnlicher Geschwindigkeit
- April 2025 Release, neuere Technologie

**Breaking Changes**: Keine - Drop-in Replacement

### Von gemini-2.5-pro zu gemini-3-pro-preview
**Warum wechseln?**
- Deutlich h√∂here Intelligenz (1501 Elo vs. ~1400 Elo)
- 1M Token Context (vs. typisch 128k)
- Bessere multimodale Capabilities
- Agentic Features

**Breaking Changes**:
- Modell-Name √§ndert sich
- L√§ngerer Name f√ºr Preview-Version

**Migration**:
```bash
# Alt
"model": "gemini-2.5-pro"

# Neu
"model": "gemini-3-pro-preview-11-2025"
```

---

## üí° Best Practices 2025

### 1. Model-Auswahl-Strategie

**Faustregel**:
- **Quality > Speed**: `o3-pro` oder `gemini-3-pro-preview`
- **Speed > Quality**: `o4-mini` oder `gemini-3.0-flash`
- **Balanced**: `gpt-5.1` oder `gemini-2.5-pro`

### 2. Context-Management

**Gro√üe Contexts (>100k tokens)**:
- Nutze `gemini-3-pro-preview-11-2025` (1M Token)
- Vermeide o3-pro (h√∂here Latenz bei gro√üen Inputs)

### 3. Cost-Optimization

**Budget-freundlich**:
1. `o4-mini` - Beste Kosten/Performance f√ºr Coding
2. `gemini-3.0-flash` - Schnellste Antworten, g√ºnstig
3. `o3-mini` - Wenn o4-mini nicht verf√ºgbar

**Premium-Performance** (h√∂here Kosten akzeptabel):
1. `o3-pro` - Wenn Qualit√§t kritisch ist
2. `gemini-3-pro-preview` - F√ºr multimodale Tasks

### 4. Debugging & Transparenz

**Wenn du den Denkprozess sehen willst**:
- Nutze `gemini-3-pro-preview-11-2025-thinking`
- Ideal f√ºr:
  - Verst√§ndnis komplexer AI-Entscheidungen
  - Debugging von fehlerhaften Antworten
  - Learning & Education

---

## üö® Bekannte Limitierungen (Stand Nov 2025)

### o3-pro
- ‚ö†Ô∏è L√§ngere Response-Times (Background-Mode empfohlen)
- ‚ö†Ô∏è Keine Image-Generation Support
- ‚ö†Ô∏è Timeout-Risiko bei Standard-Mode

### Gemini 3 Preview
- ‚ö†Ô∏è Preview-Status - API kann sich √§ndern
- ‚ö†Ô∏è Modell-Name ist tempor√§r (wird zu `gemini-3-pro` stabilisiert)
- ‚ö†Ô∏è Knowledge Cutoff: Januar 2025

### gpt-5.1
- ‚ÑπÔ∏è Noch keine offizielle Deprecation von GPT-4 angek√ºndigt
- ‚ÑπÔ∏è Legacy-Code k√∂nnte noch GPT-4-Namen verwenden

---

## üìö Weitere Ressourcen

### Offizielle Dokumentation
- **OpenAI o3**: https://openai.com/index/introducing-o3-and-o4-mini/
- **GPT-5.1**: https://help.openai.com/en/articles/11909943-gpt-5-in-chatgpt
- **Gemini 3**: https://blog.google/products/gemini/gemini-3/
- **Gemini 3 Developers**: https://blog.google/technology/developers/gemini-3-developers/

### Benchmarks & Vergleiche
- **LMArena Leaderboard**: https://lmarena.ai (Gemini 3: 1501 Elo)
- **SWE-bench**: https://www.swebench.com (o3-pro: 71.7%)
- **OpenAI Model Comparison**: https://platform.openai.com/docs/models

### Plugin-Dokumentation
- **README**: `/home/ralle/claude-code-multimodel/plugins/multi-provider-cli-chat/README.md`
- **CLAUDE.md Integration**: `/home/ralle/CLAUDE.md` (Zeile 151-185)
- **Troubleshooting**: `/home/ralle/claude-code-multimodel/PLUGIN_TROUBLESHOOTING.md`

---

## üîÑ Update-Historie

- **21.11.2025**: Initial documentation mit allen 2025 Model-Releases
  - OpenAI: o3-pro, o4-mini, gpt-5.1
  - Gemini: gemini-3-pro-preview, gemini-3.0-flash
  - Use-Case-Matrix und Best Practices hinzugef√ºgt

---

*F√ºr Aktualisierungen dieser Datei siehe: Web-Recherche zu neuesten Model-Releases*
*Plugin-Version: 0.1.0*
